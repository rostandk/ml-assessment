{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4acc1fc1",
      "metadata": {},
      "source": [
        "<a id='intro'></a>\n",
        "## Introduction\n",
        "\n",
        "This Colab notebook demonstrates an end-to-end, multimodal keyword spam moderation workflow. It combines text and (when available) images to predict a strict JSON response with three fields: `is_spam` (boolean), `confidence` (0\u20131), and a concise `reason`. The approach pairs a simple text baseline (TF\u2013IDF + logistic regression) with a fine\u2011tuned vision\u2011language model (Qwen3\u2011VL via Unsloth QLoRA). The notebook tells a clear story: load data, prepare a supervised fine\u2011tuning (SFT) dataset, train, run deterministic inference, evaluate policy thresholds (keep/review/demote), and package artifacts.\n",
        "\n",
        "Highlights:\n",
        "- Reproducible flow: the notebook delegates heavy work to a small Python package (`depop`) for clarity and testability.\n",
        "- Practical caching: images are bootstrapped from a published ZIP when available; missing assets fall back to text\u2011only prompts.\n",
        "- Policy evaluation: threshold sweep and curated gallery to review outcomes across TP/TN/FP/FN examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9915a030",
      "metadata": {},
      "source": [
        "<a id='toc'></a>\n",
        "**Table of Contents**\n",
        "1. [Introduction](#intro)\n",
        "2. [Environment Setup](#env-setup)\n",
        "3. [Old Way Review](#old-way)\n",
        "4. [Data Preparation](#data-prep)\n",
        "5. [Baseline](#baseline)\n",
        "6. [SFT Dataset](#sft)\n",
        "7. [Fine-tuning](#train)\n",
        "8. [Inference](#infer)\n",
        "9. [Evaluation](#eval)\n",
        "10. [Gallery](#gallery)\n",
        "11. [Artifacts](#artifacts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea9971fc",
      "metadata": {},
      "source": [
        "<a id='env-setup'></a>\n",
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb39d4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install dependencies (latest)\n",
        "%%capture\n",
        "!pip install -U transformers accelerate datasets trl unsloth bitsandbytes peft pillow<12 pandas scikit-learn pyarrow tqdm google-cloud-storage ipywidgets seaborn requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec3740e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Clone repository (fetch code + data)\n",
        "import os, sys, subprocess, pathlib\n",
        "REPO_URL = 'https://github.com/rostandk/ml-assessment.git'\n",
        "REPO_DIR = '/content/ml-assessment'\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    subprocess.run(['git','clone','--depth','1',REPO_URL, REPO_DIR], check=True)\n",
        "else:\n",
        "    subprocess.run(['git','-C', REPO_DIR, 'pull','--ff-only'], check=True)\n",
        "os.chdir(REPO_DIR)\n",
        "if REPO_DIR not in sys.path: sys.path.insert(0, REPO_DIR)\n",
        "print('Repository ready at:', REPO_DIR)\n",
        "print('Data directory:', os.path.join(REPO_DIR, 'data'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dffca971",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Clone repository and initialise workflow helpers\n",
        "from depop.settings import load_settings, setup_logging\n",
        "from depop.repo import RepoManager\n",
        "from depop.cache import CacheManager\n",
        "from depop.data import DataModule, BaselineModel, SFTDatasetBuilder\n",
        "from depop.training import QwenTrainer\n",
        "from depop.inference import InferenceRunner\n",
        "from depop.evaluation import EvaluationSuite\n",
        "from depop.artifacts import ArtifactManager\n",
        "\n",
        "setup_logging()\n",
        "settings = load_settings()\n",
        "print(settings.summary())\n",
        "\n",
        "repo_manager = RepoManager(settings)\n",
        "\n",
        "cache_manager = CacheManager(settings)\n",
        "data_module = DataModule(settings)\n",
        "baseline_model = BaselineModel(settings)\n",
        "sft_builder = SFTDatasetBuilder(settings, cache_manager.media_cache)\n",
        "qwen_trainer = QwenTrainer(settings)\n",
        "inference_runner = InferenceRunner(settings, cache_manager.media_cache)\n",
        "evaluator = EvaluationSuite(settings)\n",
        "artifact_manager = ArtifactManager(settings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0163cd0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Environment configuration & RNG seeds\n",
        "import json\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "RNG_SEED = settings.seed\n",
        "random.seed(RNG_SEED)\n",
        "np.random.seed(RNG_SEED)\n",
        "torch.manual_seed(RNG_SEED)\n",
        "torch.cuda.manual_seed_all(RNG_SEED)\n",
        "\n",
        "GPU_NAME = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "print(f\"Detected accelerator: {GPU_NAME}\")\n",
        "print(json.dumps({\n",
        "    \"repo_dir\": str(settings.paths.repo_dir),\n",
        "    \"artifacts_dir\": str(settings.paths.artifacts_dir),\n",
        "    \"cache_dir\": str(settings.paths.cache_dir),\n",
        "}, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7c9c2d",
      "metadata": {},
      "source": [
        "<a id='old-way'></a>\n",
        "## 2. Old Way Review\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2077a54",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Run plan overview\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "mode_hint = \"A100\" if \"A100\" in GPU_NAME else (\"T4\" if \"T4\" in GPU_NAME else \"CPU\")\n",
        "batch_size = settings.training.batch_size_a100 if mode_hint == \"A100\" else settings.training.batch_size_t4\n",
        "grad_accum = settings.training.grad_accum_a100 if mode_hint == \"A100\" else settings.training.grad_accum_t4\n",
        "\n",
        "rows = [\n",
        "    (\"Mode\", mode_hint),\n",
        "    (\"Batch size\", batch_size),\n",
        "    (\"Gradient accumulation\", grad_accum),\n",
        "    (\"Learning rate\", settings.training.learning_rate),\n",
        "    (\"Epochs\", settings.training.epochs),\n",
        "    (\"Max sequence length\", settings.training.max_seq_len),\n",
        "]\n",
        "html = \"<table><tbody>\" + \"\".join(\n",
        "    f\"<tr><th style='text-align:left;padding-right:12px;'>{k}</th><td>{v}</td></tr>\" for k, v in rows\n",
        ") + \"</tbody></table>\"\n",
        "display(HTML(html))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd24017",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Load TSVs, validate schema, compute label confidence\n",
        "train_df = data_module.load_training_dataframe()\n",
        "print(f\"Loaded {len(train_df)} training rows\")\n",
        "\n",
        "train_split, val_split = data_module.train_val_split(train_df)\n",
        "print(f\"Train rows: {len(train_split)}, Validation rows: {len(val_split)}\")\n",
        "\n",
        "try:\n",
        "    test_df = data_module.load_test_dataframe()\n",
        "    print(f\"Loaded {len(test_df)} test rows\")\n",
        "except Exception:\n",
        "    import pandas as pd\n",
        "    test_df = pd.DataFrame(columns=train_df.columns)\n",
        "    print(\"Test TSV not found; skipping test evaluation\")\n",
        "\n",
        "train_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e5b867",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Old way failure examples\n",
        "from IPython.display import display\n",
        "display(evaluator.show_legacy_failures(train_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b74cad",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Prepare cache (download images or bootstrap)\n",
        "import pandas as pd\n",
        "\n",
        "all_urls = pd.concat([\n",
        "    train_split[\"image_url\"],\n",
        "    val_split[\"image_url\"],\n",
        "    test_df.get(\"image_url\", pd.Series([], dtype=str)),\n",
        "]).dropna().unique()\n",
        "\n",
        "print(f\"Total unique URLs: {len(all_urls)}\")\n",
        "status_df = cache_manager.ensure(all_urls)\n",
        "status_path = settings.paths.artifacts_dir / \"image_download_status.csv\"\n",
        "status_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "status_df.to_csv(status_path, index=False)\n",
        "print(status_df[\"downloaded\"].value_counts())\n",
        "print(f\"Saved download status to {status_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3545310",
      "metadata": {},
      "source": [
        "<a id='data-prep'></a>\n",
        "## 3. Data Preparation\n",
        "\n",
        "We model `is_spam` (binary) with an associated `confidence` in [0,1]. We compute `label_confidence = (yes - no) / (yes + no)` as a weak indicator of label certainty. Operationally, we use two thresholds over the model's confidence to map predictions into actions: keep, review, and demote. We later sweep thresholds on validation to choose a sensible operating point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6316c70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class balance and label confidence distribution\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "train_df['label'].value_counts().sort_index().plot(kind='bar', ax=axes[0], title='Class counts (train)')\n",
        "axes[0].set_xticklabels(['non-spam', 'spam'], rotation=0)\n",
        "sns.histplot(train_df['label_confidence'], bins=20, ax=axes[1])\n",
        "axes[1].set_title('Label confidence (train)')\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d74177",
      "metadata": {},
      "source": [
        "\n",
        "#@title Train leakage-free baseline\n",
        "import json\n",
        "\n",
        "baseline_metrics = baseline_model.run(train_split, val_split)\n",
        "baseline_metrics_path = settings.paths.artifacts_dir / \"baseline_metrics.json\"\n",
        "baseline_metrics_path.write_text(json.dumps(baseline_metrics, indent=2))\n",
        "print(json.dumps(baseline_metrics, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9674513",
      "metadata": {},
      "source": [
        "<a id='baseline'></a>\n",
        "## 4. Baseline \u2013 TF-IDF + logistic regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d36c02",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Train leakage-free baseline\n",
        "import json\n",
        "\n",
        "baseline = BaselineModel(settings)\n",
        "baseline_metrics = baseline.run(train_split, val_split)\n",
        "baseline_metrics_path = settings.paths.artifacts_dir / \"baseline_metrics.json\"\n",
        "baseline_metrics_path.write_text(json.dumps(baseline_metrics, indent=2))\n",
        "print(json.dumps(baseline_metrics, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6838ef3c",
      "metadata": {},
      "source": [
        "<a id='sft'></a>\n",
        "## 5. SFT Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3992687",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Construct Unsloth-ready JSONL files\n",
        "sft_builder = SFTDatasetBuilder(settings, cache_manager)\n",
        "sft_dataset = sft_builder.build(train_split, val_split)\n",
        "print(f\"SFT rows -> train: {len(sft_dataset.train_rows)}, val: {len(sft_dataset.val_rows)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6434ba0",
      "metadata": {},
      "source": [
        "\n",
        "#@title Fine-tune with Unsloth QLoRA\n",
        "train_summary = qwen_trainer.train(sft_dataset)\n",
        "print(train_summary[\"train_result\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8381fa8",
      "metadata": {},
      "source": [
        "<a id='train'></a>\n",
        "## 6. Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7714d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Fine-tune with Unsloth QLoRA\n",
        "trainer = QwenTrainer(settings)\n",
        "train_summary = trainer.train(sft_dataset)\n",
        "print(train_summary[\"train_result\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cad85b",
      "metadata": {},
      "source": [
        "\n",
        "#@title Deterministic inference on validation (and optional test)\n",
        "val_predictions = inference_runner.predict(val_split)\n",
        "val_predictions.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb523b8",
      "metadata": {},
      "source": [
        "<a id='infer'></a>\n",
        "## 7. Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acbdc43",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Deterministic inference on validation (and optional test)\n",
        "inference = InferenceRunner(settings, cache_manager)\n",
        "val_predictions = inference.predict(val_split)\n",
        "val_predictions.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccc6418",
      "metadata": {},
      "source": [
        "<a id='eval'></a>\n",
        "## 8. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe2fba8",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Threshold sweep, demotion policy, and metrics\n",
        "import json\n",
        "\n",
        "evaluator = EvaluationSuite(settings)\n",
        "best_review, best_demote, best_score = evaluator.threshold_sweep(val_predictions)\n",
        "print(f\"Best thresholds: review={best_review:.2f}, demote={best_demote:.2f}, macro_f1={best_score:.3f}\")\n",
        "\n",
        "metrics = evaluator.evaluate(val_predictions, best_review, best_demote)\n",
        "metrics_with_thresholds = {**metrics, 'review_threshold': best_review, 'demote_threshold': best_demote}\n",
        "artifact_manager.save_metrics(metrics_with_thresholds)\n",
        "artifact_manager.save_classification_report(metrics['classification_report'])\n",
        "print(json.dumps({k: v for k, v in metrics_with_thresholds.items() if k != 'classification_report'}, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dca9d2",
      "metadata": {},
      "source": [
        "<a id='gallery'></a>\n",
        "## 9. Curated Gallery\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6306c744",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title TP/TN/FP/FN examples with images\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "gallery_html = evaluator.build_gallery(val_predictions, train_df)\n",
        "display(HTML(gallery_html))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e9b677",
      "metadata": {},
      "source": [
        "<a id='artifacts'></a>\n",
        "## 10. Package Artifacts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd8683e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Bundle outputs for download\n",
        "artifact_manager.save_predictions(val_predictions)\n",
        "package_path = artifact_manager.package()\n",
        "print(f\"Artifacts packaged at {package_path}\")\n",
        "\n",
        "published_zip = cache_manager.publish_if_enabled()\n",
        "if published_zip:\n",
        "    print(f\"Cache ZIP ready at {published_zip}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}