{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acc1fc1",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "This Colab notebook demonstrates an end-to-end, multimodal keyword spam moderation workflow. It combines text and (when available) images to predict a strict JSON response with three fields: `is_spam` (boolean), `confidence` (0–1), and a concise `reason`. The approach pairs a simple text baseline (TF–IDF + logistic regression) with a fine‑tuned vision‑language model (Qwen3‑VL via Unsloth QLoRA). The notebook tells a clear story: load data, prepare a supervised fine‑tuning (SFT) dataset, train, run deterministic inference, evaluate policy thresholds (keep/review/demote), and package artifacts.\n",
    "\n",
    "Highlights:\n",
    "- Reproducible flow: the notebook delegates heavy work to a small Python package (`depop`) for clarity and testability.\n",
    "- Practical caching: images are bootstrapped from a published ZIP when available; missing assets fall back to text‑only prompts.\n",
    "- Policy evaluation: threshold sweep and curated gallery to review outcomes across TP/TN/FP/FN examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915a030",
   "metadata": {},
   "source": [
    "<a id='toc'></a>\n",
    "**Table of Contents**\n",
    "1. [Introduction](#intro)\n",
    "2. [Environment Setup](#env-setup)\n",
    "3. [Old Way Review](#old-way)\n",
    "4. [Data Preparation](#data-prep)\n",
    "5. [Baseline](#baseline)\n",
    "6. [SFT Dataset](#sft)\n",
    "7. [Fine-tuning](#train)\n",
    "8. [Inference](#infer)\n",
    "9. [Evaluation](#eval)\n",
    "10. [Gallery](#gallery)\n",
    "11. [Artifacts](#artifacts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9971fc",
   "metadata": {},
   "source": [
    "<a id='env-setup'></a>\n",
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "runtime_toggles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Runtime toggles\n",
    "GCS_UPLOADS_ENABLED = True  #@param {type:\"boolean\"}\n",
    "MODEL_ID_OVERRIDE = \"\"  #@param {type:\"string\"}\n",
    "DTYPE_OVERRIDE = \"auto\"  #@param [\"auto\", \"bfloat16\", \"float16\"]\n",
    "\n",
    "USER_SETTINGS_OVERRIDES = {\n",
    "    \"imagestore_enabled\": GCS_UPLOADS_ENABLED,\n",
    "}\n",
    "if MODEL_ID_OVERRIDE.strip():\n",
    "    USER_SETTINGS_OVERRIDES[\"model_id\"] = MODEL_ID_OVERRIDE.strip()\n",
    "if DTYPE_OVERRIDE not in (None, \"\", \"auto\"):\n",
    "    USER_SETTINGS_OVERRIDES[\"dtype\"] = DTYPE_OVERRIDE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb39d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies (latest)\n",
    "%%capture\n",
    "!pip install -U transformers accelerate datasets trl unsloth bitsandbytes peft pillow<12 pandas scikit-learn pyarrow tqdm google-cloud-storage ipywidgets seaborn requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81519da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Authenticate with Google Cloud (required for GCS upload)\n",
    "if not globals().get(\"GCS_UPLOADS_ENABLED\", True):\n",
    "    print(\"GCS uploads disabled; skipping authentication.\")\n",
    "else:\n",
    "    try:\n",
    "        from google.colab import auth as colab_auth\n",
    "        colab_auth.authenticate_user()\n",
    "        print('Authenticated with Google Cloud')\n",
    "    except Exception as e:\n",
    "        print('GCP auth step skipped or not available:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Clone repository (fetch code + data)\n",
    "import os, sys, subprocess, pathlib\n",
    "REPO_URL = 'https://github.com/rostandk/ml-assessment.git'\n",
    "REPO_DIR = '/content/ml-assessment'\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    subprocess.run(['git','clone','--depth','1',REPO_URL, REPO_DIR], check=True)\n",
    "else:\n",
    "    subprocess.run(['git','-C', REPO_DIR, 'pull','--ff-only'], check=True)\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path: sys.path.insert(0, REPO_DIR)\n",
    "print('Repository ready at:', REPO_DIR)\n",
    "print('Data directory:', os.path.join(REPO_DIR, 'data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffca971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Clone repository and initialise workflow helpers\n",
    "from depop.settings import load_settings, setup_logging\n",
    "from depop.repo import RepoManager\n",
    "from depop.media import ImageStore\n",
    "from depop.data import DataModule, BaselineModel, SFTDatasetBuilder\n",
    "from depop.training import QwenTrainer\n",
    "from depop.inference import InferenceRunner\n",
    "from depop.evaluation import EvaluationSuite\n",
    "from depop.artifacts import ArtifactManager\n",
    "\n",
    "setup_logging()\n",
    "overrides = globals().get(\"USER_SETTINGS_OVERRIDES\", {})\n",
    "settings = load_settings(overrides)\n",
    "print(settings.summary())\n",
    "\n",
    "repo_manager = RepoManager(settings)\n",
    "\n",
    "artifact_manager = ArtifactManager(settings)\n",
    "image_store = ImageStore(\n",
    "    settings.paths.cache_dir,\n",
    "    gcs_bucket=settings.gcs.bucket,\n",
    "    gcs_images_prefix=settings.gcs.images_prefix,\n",
    ")\n",
    "data_module = DataModule(settings)\n",
    "baseline_model = BaselineModel(settings)\n",
    "sft_builder = SFTDatasetBuilder(settings, image_store, artifact_manager)\n",
    "qwen_trainer = QwenTrainer(settings, artifact_manager)\n",
    "inference_runner = InferenceRunner(settings, image_store)\n",
    "evaluator = EvaluationSuite(settings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Environment configuration & RNG seeds\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "RNG_SEED = settings.seed\n",
    "random.seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "torch.cuda.manual_seed_all(RNG_SEED)\n",
    "\n",
    "GPU_NAME = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
    "print(f'Detected accelerator: {GPU_NAME}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c9c2d",
   "metadata": {},
   "source": [
    "<a id='old-way'></a>\n",
    "## 2. Old Way Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Run plan overview\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "mode_hint = \"A100\" if \"A100\" in GPU_NAME else (\"T4\" if \"T4\" in GPU_NAME else \"CPU\")\n",
    "batch_size = settings.training.batch_size_a100 if mode_hint == \"A100\" else settings.training.batch_size_t4\n",
    "grad_accum = settings.training.grad_accum_a100 if mode_hint == \"A100\" else settings.training.grad_accum_t4\n",
    "\n",
    "rows = [\n",
    "    (\"Mode\", mode_hint),\n",
    "    (\"Batch size\", batch_size),\n",
    "    (\"Gradient accumulation\", grad_accum),\n",
    "    (\"Learning rate\", settings.training.learning_rate),\n",
    "    (\"Epochs\", settings.training.epochs),\n",
    "    (\"Max sequence length\", settings.training.max_seq_len),\n",
    "]\n",
    "html = \"<table><tbody>\" + \"\".join(\n",
    "    f\"<tr><th style='text-align:left;padding-right:12px;'>{k}</th><td>{v}</td></tr>\" for k, v in rows\n",
    ") + \"</tbody></table>\"\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd24017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load TSVs, validate schema, compute label confidence\n",
    "from IPython.display import display\n",
    "\n",
    "train_df = data_module.load_training_dataframe()\n",
    "print(f\"Loaded {len(train_df)} training rows\")\n",
    "\n",
    "train_split, val_split = data_module.train_val_split(train_df)\n",
    "print(f\"Train rows: {len(train_split)}, Validation rows: {len(val_split)}\")\n",
    "\n",
    "try:\n",
    "    test_df = data_module.load_test_dataframe()\n",
    "    print(f\"Loaded {len(test_df)} test rows\")\n",
    "except Exception:\n",
    "    import pandas as pd\n",
    "    test_df = pd.DataFrame(columns=train_df.columns)\n",
    "    print(\"Test TSV not found; skipping test evaluation\")\n",
    "\n",
    "display(train_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Old way failure examples\n",
    "from IPython.display import display\n",
    "display(evaluator.show_legacy_failures(train_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b74cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Prepare cache (download + upload to GCS + manifest)\n",
    "import pandas as pd\n",
    "\n",
    "all_urls = pd.concat([\n",
    "    train_split[\"image_url\"],\n",
    "    val_split[\"image_url\"],\n",
    "    test_df.get(\"image_url\", pd.Series([], dtype=str)),\n",
    "], axis=0).dropna().unique()\n",
    "\n",
    "print(f\"Total unique URLs: {len(all_urls)}\")\n",
    "manifest_df = pd.DataFrame(image_store.ensure_all(all_urls))\n",
    "manifest_path = artifact_manager.save_manifest(manifest_df)\n",
    "if manifest_df.empty:\n",
    "    summary = {\"downloaded\": 0, \"uploaded\": 0, \"failed\": 0}\n",
    "else:\n",
    "    downloaded = manifest_df[\"downloaded\"].astype(bool)\n",
    "    uploaded = manifest_df[\"uploaded\"].astype(bool)\n",
    "    summary = {\n",
    "        \"downloaded\": int(downloaded.sum()),\n",
    "        \"uploaded\": int(uploaded.sum()),\n",
    "        \"failed\": int((~downloaded).sum()),\n",
    "    }\n",
    "print(summary)\n",
    "print(f\"Image manifest saved to {manifest_path}\")\n",
    "if not manifest_df.empty:\n",
    "    uploaded_urls = manifest_df.loc[manifest_df[\"uploaded\"].astype(bool), \"public_url\"].dropna()\n",
    "    sample_url = uploaded_urls.iloc[0] if not uploaded_urls.empty else \"N/A\"\n",
    "    print(f\"Sample public URL: {sample_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3545310",
   "metadata": {},
   "source": [
    "<a id='data-prep'></a>\n",
    "## 3. Data Preparation\n",
    "\n",
    "We model `is_spam` (binary) with an associated `confidence` in [0,1]. We compute `label_confidence = (yes - no) / (yes + no)` as a weak indicator of label certainty. Operationally, we use two thresholds over the model's confidence to map predictions into actions: keep, review, and demote. We later sweep thresholds on validation to choose a sensible operating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6316c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance and label confidence distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "train_df['label'].value_counts().sort_index().plot(kind='bar', ax=axes[0], title='Class counts (train)')\n",
    "axes[0].set_xticklabels(['non-spam', 'spam'], rotation=0)\n",
    "sns.histplot(train_df['label_confidence'], bins=20, ax=axes[1])\n",
    "axes[1].set_title('Label confidence (train)')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d74177",
   "metadata": {},
   "source": [
    "Our baseline keeps us honest: a TF-IDF + logistic regression model trained on the text-only features. It establishes a cheap, leakage-free benchmark that we log to `baseline_metrics.json` for later comparison with the VLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9674513",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "## 4. Baseline – TF-IDF + logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d36c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train leakage-free baseline\n",
    "import json\n",
    "\n",
    "baseline_metrics = baseline_model.run(train_split, val_split)\n",
    "baseline_metrics_path = settings.paths.artifacts_dir / \"baseline_metrics.json\"\n",
    "baseline_metrics_path.write_text(json.dumps(baseline_metrics, indent=2))\n",
    "print(json.dumps(baseline_metrics, indent=2))\n",
    "print(f\"Baseline metrics saved to {baseline_metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838ef3c",
   "metadata": {},
   "source": [
    "<a id='sft'></a>\n",
    "## 5. SFT Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3992687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Construct Unsloth-ready JSONL files\n",
    "sft_dataset = sft_builder.build(train_split, val_split)\n",
    "print(f\"SFT rows -> train: {len(sft_dataset.train_rows)}, val: {len(sft_dataset.val_rows)}\")\n",
    "print(f\"JSONL saved to {sft_dataset.train_path} and {sft_dataset.val_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6434ba0",
   "metadata": {},
   "source": [
    "Fine-tuning uses Unsloth's QLoRA recipe on Qwen3-VL. The helper automatically sets batch size / grad accumulation based on the detected GPU and uploads the merged adapter to GCS once training finishes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8381fa8",
   "metadata": {},
   "source": [
    "<a id='train'></a>\n",
    "## 6. Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7714d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Fine-tune with Unsloth QLoRA\n",
    "import json\n",
    "\n",
    "train_summary = qwen_trainer.train(sft_dataset)\n",
    "print(json.dumps(train_summary[\"train_result\"], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cad85b",
   "metadata": {},
   "source": [
    "Inference reuses the fine-tuned model with deterministic decoding. When an image is missing we fall back to the text-only prompt (the manifest still records which URLs were unavailable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb523b8",
   "metadata": {},
   "source": [
    "<a id='infer'></a>\n",
    "## 7. Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Deterministic inference on validation (and optional test)\n",
    "val_predictions = inference_runner.predict(val_split)\n",
    "val_predictions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc6418",
   "metadata": {},
   "source": [
    "<a id='eval'></a>\n",
    "## 8. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Threshold sweep, demotion policy, and metrics\n",
    "import json\n",
    "\n",
    "best_review, best_demote, best_score = evaluator.threshold_sweep(val_predictions)\n",
    "print(f\"Best thresholds: review={best_review:.2f}, demote={best_demote:.2f}, macro_f1={best_score:.3f}\")\n",
    "\n",
    "metrics = evaluator.evaluate(val_predictions, best_review, best_demote)\n",
    "metrics_with_thresholds = {**metrics, 'review_threshold': best_review, 'demote_threshold': best_demote}\n",
    "artifact_manager.save_metrics(metrics_with_thresholds)\n",
    "artifact_manager.save_classification_report(metrics['classification_report'])\n",
    "print(json.dumps({k: v for k, v in metrics_with_thresholds.items() if k != 'classification_report'}, indent=2))\n",
    "print(f\"Metrics saved to {settings.paths.metrics_path}\")\n",
    "print(f\"Classification report saved to {settings.paths.classification_report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dca9d2",
   "metadata": {},
   "source": [
    "<a id='gallery'></a>\n",
    "## 9. Curated Gallery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title TP/TN/FP/FN examples with images\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "gallery_html = evaluator.build_gallery(val_predictions, train_df)\n",
    "display(HTML(gallery_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9b677",
   "metadata": {},
   "source": [
    "<a id='artifacts'></a>\n",
    "## 10. Package Artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8683e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Bundle outputs for download\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "artifact_manager.save_predictions(val_predictions)\n",
    "package_path = artifact_manager.package()\n",
    "print(f\"Artifacts packaged at {package_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
