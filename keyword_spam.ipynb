{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Keyword Spam Moderation – Colab Notebook\n",
        "\n",
        "This notebook implements the workflow described in `docs/plan_collab.md`. Run the\n",
        "cells top-to-bottom on Google Colab (GPU runtime; A100 preferred, T4 works with\n",
        "smaller batches).\n",
        "\n",
        "> **Before you start**\n",
        "> 1. Upload `data/train_set.tsv` and `data/test_set.tsv` somewhere accessible\n",
        ">    (Drive or the Colab file system).\n",
        "> 2. Provide a Hugging Face token if the chosen model is gated.\n",
        "> 3. Optionally enable the cache export flags later if you want to persist\n",
        ">    downloaded images to Drive or GCS."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ac43bf",
      "metadata": {},
      "source": [
        "## Executive Introduction\n",
        "\n",
        "This notebook delivers an end-to-end multimodal moderation workflow for detecting keyword/brand spamming in marketplace listings. We predict a strict JSON object with three fields: `is_spam` (boolean), `confidence` (0-1), and a concise `reason`.\n",
        "\n",
        "Why this approach:\n",
        "- Problem: keyword/brand stuffing erodes trust and hurts user experience.\n",
        "- Target: a calibrated binary signal (is_spam) with confidence that maps to policy actions (keep/review/demote).\n",
        "- Model: Qwen/Qwen3-VL-2B-Instruct via Unsloth QLoRA fits Colab GPUs while retaining strong multimodal reasoning.\n",
        "- Simplicity: Transformers inference keeps runs reproducible without servers.\n",
        "\n",
        "We compare against a leakage-free TF-IDF baseline, fine-tune the VLM, evaluate and sweep thresholds, and package artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dffca971",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install dependencies (pinned to avoid Colab drift)\n",
        "%%capture\n",
        "!pip install -U \"transformers==4.44.2\" \"accelerate==0.34.2\" \"peft==0.12.0\" \"datasets==2.20.0\" \\\n",
        "               unsloth bitsandbytes pillow pandas scikit-learn pyarrow tqdm trl \\\n",
        "               google-cloud-storage ipywidgets seaborn requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0559e1e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Clone repository and set paths\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "REPO_URL = 'https://github.com/rostandk/ml-assessment.git'\n",
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "DEFAULT_REPO_DIR = Path('/content/ml-assessment')\n",
        "LOCAL_REPO_DIR = Path.cwd()\n",
        "\n",
        "def ensure_repo(url: str, target: Path) -> Path:\n",
        "    \"\"\"Clone *url* to *target* if missing, otherwise fast-forward.\"\"\"\n",
        "    if target.exists():\n",
        "        subprocess.run(['git', '-C', str(target), 'pull', '--ff-only'], check=True)\n",
        "    else:\n",
        "        target.parent.mkdir(parents=True, exist_ok=True)\n",
        "        subprocess.run(['git', 'clone', '--depth', '1', url, str(target)], check=True)\n",
        "    return target.resolve()\n",
        "\n",
        "REPO_DIR = ensure_repo(REPO_URL, DEFAULT_REPO_DIR) if IS_COLAB else LOCAL_REPO_DIR\n",
        "\n",
        "if IS_COLAB:\n",
        "    os.chdir(REPO_DIR)\n",
        "\n",
        "if str(REPO_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_DIR))\n",
        "\n",
        "DATA_DIR = REPO_DIR / 'data'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3133cda",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title (Optional) Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0163cd0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Imports & configuration\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Any, Iterable\n",
        "\n",
        "try:\n",
        "    from google.colab import output\n",
        "    output.clear(wait=True)\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import HTML, display\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastVisionModel, UnslothVisionDataCollator\n",
        "from transformers import EarlyStoppingCallback, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "import torch\n",
        "\n",
        "import sys\n",
        "import pathlib\n",
        "\n",
        "REPO_DIR = globals().get('REPO_DIR', pathlib.Path().resolve())\n",
        "if str(REPO_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_DIR))\n",
        "\n",
        "import utils\n",
        "\n",
        "RNG_SEED = 42\n",
        "random.seed(RNG_SEED)\n",
        "np.random.seed(RNG_SEED)\n",
        "torch.manual_seed(RNG_SEED)\n",
        "torch.cuda.manual_seed_all(RNG_SEED)\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    DEFAULT_CACHE_DIR = Path('/content/cache/images')\n",
        "    DEFAULT_ARTIFACTS_DIR = Path('/content/artifacts')\n",
        "    DEFAULT_DRIVE_CACHE_PATH = Path('/content/drive/MyDrive/keyword_spam_vlm/cache/images')\n",
        "else:\n",
        "    DEFAULT_CACHE_DIR = Path('cache/images')\n",
        "    DEFAULT_ARTIFACTS_DIR = Path('artifacts')\n",
        "    DEFAULT_DRIVE_CACHE_PATH = Path.home() / 'keyword_spam_vlm' / 'cache' / 'images'\n",
        "\n",
        "DATA_DIR = globals().get('DATA_DIR', REPO_DIR / 'data')\n",
        "TRAIN_TSV_PATH = (DATA_DIR / 'train_set.tsv').resolve()\n",
        "TEST_TSV_PATH = (DATA_DIR / 'test_set.tsv').resolve()\n",
        "\n",
        "CONFIG = {\n",
        "    'model_id': 'Qwen/Qwen3-VL-2B-Instruct',\n",
        "    'dtype': 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16',\n",
        "    'epochs': 3,\n",
        "    'learning_rate': 1e-4,\n",
        "    'max_seq_len': 1024,\n",
        "    'warmup_ratio': 0.05,\n",
        "    'batch_size_t4': 4,\n",
        "    'grad_accum_t4': 4,\n",
        "    'batch_size_a100': 8,\n",
        "    'grad_accum_a100': 2,\n",
        "    'review_threshold': 0.5,\n",
        "    'demote_threshold': 0.7,\n",
        "    'cache_dir': str(DEFAULT_CACHE_DIR),\n",
        "    'train_tsv': str(TRAIN_TSV_PATH),\n",
        "    'test_tsv': str(TEST_TSV_PATH),\n",
        "    'artifacts_dir': str(DEFAULT_ARTIFACTS_DIR),\n",
        "    'export_cache_to_drive': False,\n",
        "    'drive_cache_path': str(DEFAULT_DRIVE_CACHE_PATH),\n",
        "    'export_cache_to_gcs': False,\n",
        "    'gcs_bucket': 'ml-assesment',\n",
        "    'gcs_prefix': 'images',\n",
        "}\n",
        "\n",
        "Path(CONFIG['cache_dir']).mkdir(parents=True, exist_ok=True)\n",
        "Path(CONFIG['artifacts_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "GPU_NAME = torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'\n",
        "print(f'Detected accelerator: {GPU_NAME}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Run plan overview\n",
        "from IPython.display import HTML\n",
        "\n",
        "per_device_batch = CONFIG[\"batch_size_a100\"] if \"A100\" in GPU_NAME else CONFIG[\"batch_size_t4\"]\n",
        "grad_accum = CONFIG[\"grad_accum_a100\"] if \"A100\" in GPU_NAME else CONFIG[\"grad_accum_t4\"]\n",
        "mode_hint = \"Full run recommended\" if \"A100\" in GPU_NAME else \"Start with a smaller subset (`max_rows`)\"\n",
        "\n",
        "rows = [\n",
        "    (\"Accelerator\", GPU_NAME),\n",
        "    (\"Per-device batch size\", per_device_batch),\n",
        "    (\"Gradient accumulation\", grad_accum),\n",
        "    (\"Learning rate\", CONFIG[\"learning_rate\"]),\n",
        "    (\"Epochs\", CONFIG[\"epochs\"]),\n",
        "    (\"Max sequence length\", CONFIG[\"max_seq_len\"]),\n",
        "    (\"Suggested mode\", mode_hint),\n",
        "]\n",
        "\n",
        "html = \"<table><tbody>\" + \"\".join(\n",
        "    f\"<tr><th style='text-align:left;padding-right:12px;'>{k}</th><td>{v}</td></tr>\" for k, v in rows\n",
        ") + \"</tbody></table>\"\n",
        "display(HTML(html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data preparation and Old Way review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Load TSVs, validate schema, compute label confidence\n",
        "REQUIRED_COLUMNS = [\"product_id\", \"description\", \"image_url\", \"label\", \"yes_count\", \"no_count\"]\n",
        "\n",
        "\n",
        "def load_dataset_tsv(path: str | Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, sep=\"\t\")\n",
        "    missing = [col for col in REQUIRED_COLUMNS if col not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns: {missing}\")\n",
        "    df = df[REQUIRED_COLUMNS].copy()\n",
        "    df[\"product_id\"] = df[\"product_id\"].astype(str)\n",
        "    if df[\"product_id\"].duplicated().any():\n",
        "        raise ValueError(\"Duplicate product_id values detected\")\n",
        "    df[\"label\"] = df[\"label\"].astype(int)\n",
        "    for col in (\"yes_count\", \"no_count\"):\n",
        "        df[col] = df[col].fillna(0).astype(int)\n",
        "        if (df[col] < 0).any():\n",
        "            raise ValueError(f\"Negative values found in {col}\")\n",
        "    total_votes = df[\"yes_count\"] + df[\"no_count\"]\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        confidence = (df[\"yes_count\"] - df[\"no_count\"]) / np.where(total_votes == 0, np.nan, total_votes)\n",
        "    df[\"label_confidence\"] = confidence.fillna(0.0)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = load_dataset_tsv(CONFIG[\"train_tsv\"])\n",
        "print(f\"Loaded {len(train_df)} training rows\")\n",
        "\n",
        "train_split, val_split = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.1,\n",
        "    stratify=train_df[\"label\"],\n",
        "    random_state=RNG_SEED,\n",
        ")\n",
        "print(f\"Train rows: {len(train_split)}, Validation rows: {len(val_split)}\")\n",
        "\n",
        "try:\n",
        "    test_df = load_dataset_tsv(CONFIG[\"test_tsv\"])\n",
        "    print(f\"Loaded {len(test_df)} test rows\")\n",
        "except FileNotFoundError:\n",
        "    test_df = pd.DataFrame(columns=REQUIRED_COLUMNS)\n",
        "    print(\"Test TSV not found; skipping test evaluation\")\n",
        "\n",
        "train_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5e5b867",
      "metadata": {},
      "source": [
        "## 2. Junior Notebook Review (Old Way)\n",
        "\n",
        "Common weaknesses observed in the legacy approach:\n",
        "- Single-modality bias: images are ignored, missing brand/category cues.\n",
        "- Fragile heuristics: hashtag spam and CTAs (e.g., 'DM for deals') fool simple rules.\n",
        "- Limited reproducibility: loose splits and few guardrails.\n",
        "- Low operational tie-in: no calibrated confidence for policy thresholds.\n",
        "\n",
        "Illustrative examples below (sampled from the training data):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ee6823",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Illustrative failure taxonomy from the legacy 'Old Way'\n",
        "import re\n",
        "\n",
        "def _hashtag_ratio(text: str) -> float:\n",
        "    tokens = str(text).split()\n",
        "    return 0.0 if not tokens else sum(1 for t in tokens if t.startswith('#')) / len(tokens)\n",
        "\n",
        "CTA_RE = re.compile(r\"\\b(dm|whatsapp|contact|inbox|email|text me)\\b\", re.I)\n",
        "BRANDS = {'nike', 'adidas', 'gucci'}\n",
        "\n",
        "def _cta_present(text: str) -> bool:\n",
        "    return bool(CTA_RE.search(str(text)))\n",
        "\n",
        "def _brand_mentioned(text: str) -> bool:\n",
        "    low = str(text).lower()\n",
        "    return any(b in low for b in BRANDS)\n",
        "\n",
        "def _shorten(text: str, n: int = 120) -> str:\n",
        "    s = str(text).strip().replace('', ' ')\n",
        "    return s if len(s) <= n else s[: n - 1] + '…'\n",
        "\n",
        "legacy_rows = []\n",
        "for row in train_df.itertuples(index=False):\n",
        "    reason = None\n",
        "    if _hashtag_ratio(row.description) > 0.2:\n",
        "        reason = 'hashtag-heavy description'\n",
        "    elif _cta_present(row.description):\n",
        "        reason = 'call-to-action present'\n",
        "    elif _brand_mentioned(row.description):\n",
        "        reason = 'brand keyword mentioned'\n",
        "    if reason:\n",
        "        legacy_rows.append({\n",
        "            'product_id': row.product_id,\n",
        "            'label': int(row.label),\n",
        "            'matched_reason': reason,\n",
        "            'description_snippet': _shorten(row.description),\n",
        "        })\n",
        "    if len(legacy_rows) >= 9:  # keep it compact\n",
        "        break\n",
        "pd.DataFrame(legacy_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download images on demand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Parallel image download\n",
        "all_urls = pd.concat([\n",
        "    train_split[\"image_url\"],\n",
        "    val_split[\"image_url\"],\n",
        "    test_df.get(\"image_url\", pd.Series([], dtype=str)),\n",
        "]).dropna().unique()\n",
        "\n",
        "print(f\"Total unique URLs: {len(all_urls)}\")\n",
        "results = utils.download_images(\n",
        "    all_urls,\n",
        "    CONFIG[\"cache_dir\"],\n",
        "    max_workers=12,\n",
        "    timeout=20,\n",
        "    max_retries=4,\n",
        ")\n",
        "status_df = pd.DataFrame(results)\n",
        "status_path = Path(CONFIG[\"artifacts_dir\"]) / \"image_download_status.csv\"\n",
        "status_df.to_csv(status_path, index=False)\n",
        "print(status_df[\"downloaded\"].value_counts())\n",
        "print(f\"Saved download status to {status_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: export cache to Drive or GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Export cache (optional)\n",
        "from shutil import copytree, ignore_patterns\n",
        "\n",
        "if CONFIG[\"export_cache_to_drive\"]:\n",
        "    drive_path = Path(CONFIG[\"drive_cache_path\"])\n",
        "    if drive_path.exists():\n",
        "        print(f\"Drive path {drive_path} already exists – remove it first if you want a fresh copy\")\n",
        "    else:\n",
        "        drive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        copytree(CONFIG[\"cache_dir\"], drive_path, ignore=ignore_patterns(\"*.zip\"))\n",
        "        print(f\"Exported cache to {drive_path}\")\n",
        "\n",
        "if CONFIG[\"export_cache_to_gcs\"]:\n",
        "    utils.sync_images_to_gcs(\n",
        "        CONFIG[\"cache_dir\"],\n",
        "        bucket=CONFIG[\"gcs_bucket\"],\n",
        "        prefix=CONFIG[\"gcs_prefix\"],\n",
        "        public=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3545310",
      "metadata": {},
      "source": [
        "## 3. What We Predict\n",
        "\n",
        "We model `is_spam` (binary) with an associated `confidence` in [0,1]. We compute `label_confidence = (yes - no) / (yes + no)` as a weak indicator of label certainty. Operationally, we use two thresholds over the model's confidence to map predictions into actions: keep, review, and demote. We later sweep thresholds on validation to choose a sensible operating point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6316c70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class balance and label confidence distribution\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "train_df['label'].value_counts().sort_index().plot(kind='bar', ax=axes[0], title='Class counts (train)')\n",
        "axes[0].set_xticklabels(['non-spam', 'spam'], rotation=0)\n",
        "sns.histplot(train_df['label_confidence'], bins=20, ax=axes[1])\n",
        "axes[1].set_title('Label confidence (train)')\n",
        "plt.tight_layout(); plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d74177",
      "metadata": {},
      "source": [
        "## 4. Baseline – TF-IDF + logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Train leakage-free baseline\n",
        "\n",
        "def run_baseline(train_df: pd.DataFrame, val_df: pd.DataFrame) -> dict[str, Any]:\n",
        "    vectorizer = TfidfVectorizer(max_features=10000, stop_words=\"english\")\n",
        "    X_train = vectorizer.fit_transform(train_df[\"description\"])\n",
        "    X_val = vectorizer.transform(val_df[\"description\"])\n",
        "\n",
        "    clf = LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
        "    clf.fit(X_train, train_df[\"label\"])\n",
        "\n",
        "    preds = clf.predict(X_val)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(val_df[\"label\"], preds, average=\"macro\", zero_division=0)\n",
        "    accuracy = accuracy_score(val_df[\"label\"], preds)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"macro_precision\": float(precision),\n",
        "        \"macro_recall\": float(recall),\n",
        "        \"macro_f1\": float(f1),\n",
        "        \"classification_report\": classification_report(val_df[\"label\"], preds, digits=3),\n",
        "    }\n",
        "    (Path(CONFIG[\"artifacts_dir\"]) / \"baseline_metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
        "    return metrics\n",
        "\n",
        "\n",
        "baseline_metrics = run_baseline(train_split, val_split)\n",
        "print(json.dumps(baseline_metrics, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build SFT datasets for Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3992687",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Construct Unsloth-ready JSONL files\n",
        "PROMPT_TEMPLATE = \"You are a moderator. Respond with JSON containing is_spam (bool), confidence (0-1), reason (short).\"\n",
        "\n",
        "\n",
        "def build_messages(df: pd.DataFrame) -> list[dict[str, Any]]:\n",
        "    rows: list[dict[str, Any]] = []\n",
        "    for record in df.to_dict(\"records\"):\n",
        "        url = record.get(\"image_url\") or \"\"\n",
        "        image_path = utils.download_image(url, CONFIG[\"cache_dir\"]) if url else None\n",
        "        user_content = []\n",
        "        if image_path is not None and image_path.exists():\n",
        "            user_content.append({\"type\": \"image\", \"image\": str(image_path)})\n",
        "        message_text = f\"{PROMPT_TEMPLATE}\\n\\nDescription: {record['description']}\"\n",
        "        if not (image_path is not None and image_path.exists()):\n",
        "            message_text += \"\\n\\nNote: image unavailable. Base your judgment on the text only.\"\n",
        "        user_content.append({\"type\": \"text\", \"text\": message_text})\n",
        "        assistant_text = json.dumps(\n",
        "            {\n",
        "                \"is_spam\": bool(record[\"label\"]),\n",
        "                \"confidence\": 1.0,\n",
        "                \"reason\": \"Training label\",\n",
        "            },\n",
        "            ensure_ascii=False,\n",
        "        )\n",
        "        rows.append(\n",
        "            {\n",
        "                \"id\": record[\"product_id\"],\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"Respond with strict JSON.\"}]},\n",
        "                    {\"role\": \"user\", \"content\": user_content},\n",
        "                    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": assistant_text}]},\n",
        "                ],\n",
        "            }\n",
        "        )\n",
        "    return rows\n",
        "\n",
        "\n",
        "sft_dir = Path(CONFIG[\"artifacts_dir\"]) / \"sft\"\n",
        "sft_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "train_rows = build_messages(train_split)\n",
        "val_rows = build_messages(val_split)\n",
        "\n",
        "train_jsonl = sft_dir / \"train.jsonl\"\n",
        "val_jsonl = sft_dir / \"val.jsonl\"\n",
        "train_jsonl.write_text(\"\".join(json.dumps(r, ensure_ascii=False) for r in train_rows))\n",
        "val_jsonl.write_text(\"\".join(json.dumps(r, ensure_ascii=False) for r in val_rows))\n",
        "\n",
        "VAL_TRUE_LABELS = val_split[\"label\"].astype(int).reset_index(drop=True).tolist()\n",
        "print(f\"SFT rows -> train: {len(train_rows)}, val: {len(val_rows)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. QLoRA fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7714d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Fine-tune with Unsloth QLoRA\n",
        "NUM_GPUS = torch.cuda.device_count()\n",
        "batch_size = CONFIG['batch_size_a100' if 'A100' in GPU_NAME else 'batch_size_t4']\n",
        "grad_accum = CONFIG['grad_accum_a100' if 'A100' in GPU_NAME else 'grad_accum_t4']\n",
        "\n",
        "train_dataset = load_dataset('json', data_files={'train': str(train_jsonl), 'validation': str(val_jsonl)})\n",
        "\n",
        "model, tokenizer, image_processor = FastVisionModel.from_pretrained(\n",
        "    CONFIG['model_id'],\n",
        "    max_seq_length=CONFIG['max_seq_len'],\n",
        "    dtype=CONFIG['dtype'],\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(Path(CONFIG['artifacts_dir']) / 'trainer'),\n",
        "    num_train_epochs=CONFIG['epochs'],\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    bf16=(CONFIG['dtype'] == 'bfloat16'),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_macro_f1',\n",
        "    greater_is_better=True,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=256,\n",
        ")\n",
        "\n",
        "vision_collator = UnslothVisionDataCollator(tokenizer=tokenizer, image_processor=image_processor)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds = eval_preds.predictions\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    def extract_label(text: str) -> int:\n",
        "        try:\n",
        "            payload = json.loads(text)\n",
        "            return int(bool(payload.get('is_spam')))\n",
        "        except Exception:\n",
        "            return 0\n",
        "\n",
        "    pred_labels = [extract_label(t) for t in decoded_preds]\n",
        "    y_true = VAL_TRUE_LABELS[: len(pred_labels)]\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, pred_labels, average='macro', zero_division=0)\n",
        "    accuracy = accuracy_score(y_true, pred_labels)\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'macro_f1': f1,\n",
        "        'macro_precision': precision,\n",
        "        'macro_recall': recall,\n",
        "    }\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset['train'],\n",
        "    eval_dataset=train_dataset['validation'],\n",
        "    data_collator=vision_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n",
        "train_result = trainer.train()\n",
        "print(train_result)\n",
        "\n",
        "merged_dir = Path(CONFIG['artifacts_dir']) / 'merged_model'\n",
        "merged_dir.mkdir(parents=True, exist_ok=True)\n",
        "FastVisionModel.save_pretrained(model, merged_dir, tokenizer=tokenizer, image_processor=image_processor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Transformers inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9acbdc43",
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Deterministic inference on validation (and optional test)\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
        "\n",
        "merged_path = Path(CONFIG[\"artifacts_dir\"]) / \"merged_model\"\n",
        "processor = AutoProcessor.from_pretrained(merged_path)\n",
        "merged_model = AutoModelForVision2Seq.from_pretrained(merged_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def generate_predictions(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    outputs = []\n",
        "    for record in tqdm(df.to_dict(\"records\"), desc=\"Generating\"):\n",
        "        url = record.get(\"image_url\") or \"\"\n",
        "        image_path = utils.download_image(url, CONFIG[\"cache_dir\"]) if url else None\n",
        "        user_content = []\n",
        "        if image_path is not None and image_path.exists():\n",
        "            user_content.append({\"type\": \"image\", \"image\": str(image_path)})\n",
        "        message_text = f\"{PROMPT_TEMPLATE}\\n\\nDescription: {record['description']}\"\n",
        "        if not (image_path is not None and image_path.exists()):\n",
        "            message_text += \"\\n\\nNote: image unavailable. Base your judgment on the text only.\"\n",
        "        user_content.append({\"type\": \"text\", \"text\": message_text})\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"Strict JSON with is_spam, confidence, reason.\"}]},\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "        ]\n",
        "\n",
        "        inputs = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(merged_model.device)\n",
        "        output = merged_model.generate(**inputs, max_new_tokens=256, temperature=0.0, do_sample=False)\n",
        "        decoded = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
        "        try:\n",
        "            payload = json.loads(decoded)\n",
        "        except Exception:\n",
        "            payload = {\"is_spam\": False, \"confidence\": 0.0, \"reason\": \"malformed\"}\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"product_id\": record[\"product_id\"],\n",
        "                \"label\": record.get(\"label\", 0),\n",
        "                \"image_available\": bool(image_path and image_path.exists()),\n",
        "                \"raw_response\": decoded,\n",
        "                \"is_spam_pred\": bool(payload.get(\"is_spam\")),\n",
        "                \"confidence_pred\": float(payload.get(\"confidence\", 0.0)),\n",
        "                \"reason_pred\": payload.get(\"reason\", \"\"),\n",
        "            }\n",
        "        )\n",
        "    return pd.DataFrame(outputs)\n",
        "\n",
        "\n",
        "val_predictions = generate_predictions(val_split)\n",
        "val_predictions.to_parquet(Path(CONFIG[\"artifacts_dir\"]) / \"validation_predictions.parquet\", index=False)\n",
        "print(val_predictions.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation & policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Threshold sweep, demotion policy, and metrics\n",
        "\n",
        "def evaluate_predictions(df: pd.DataFrame, review_threshold: float, demote_threshold: float) -> dict[str, Any]:\n",
        "    y_true = df[\"label\"].astype(int).tolist()\n",
        "    y_pred = df[\"is_spam_pred\"].astype(int).tolist()\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    decisions = []\n",
        "    for row in df.itertuples(index=False):\n",
        "        if bool(row.is_spam_pred) and row.confidence_pred >= demote_threshold:\n",
        "            decisions.append(\"demote\")\n",
        "        elif bool(row.is_spam_pred) and row.confidence_pred >= review_threshold:\n",
        "            decisions.append(\"review\")\n",
        "        else:\n",
        "            decisions.append(\"keep\")\n",
        "    df = df.assign(decision=decisions)\n",
        "\n",
        "    image_present = df[df[\"image_available\"]]\n",
        "    image_missing = df[~df[\"image_available\"]]\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"macro_f1\": f1,\n",
        "        \"macro_precision\": precision,\n",
        "        \"macro_recall\": recall,\n",
        "        \"image_present_count\": int(len(image_present)),\n",
        "        \"image_missing_count\": int(len(image_missing)),\n",
        "    }\n",
        "    (Path(CONFIG[\"artifacts_dir\"]) / \"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
        "    (Path(CONFIG[\"artifacts_dir\"]) / \"classification_report.json\").write_text(classification_report(y_true, y_pred, digits=3))\n",
        "    df.to_parquet(Path(CONFIG[\"artifacts_dir\"]) / \"predictions_with_decisions.parquet\", index=False)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def sweep_thresholds(df: pd.DataFrame, review_grid: Iterable[float], demote_grid: Iterable[float]) -> tuple[float, float, float]:\n",
        "    best_score = -1.0\n",
        "    best_review = CONFIG[\"review_threshold\"]\n",
        "    best_demote = CONFIG[\"demote_threshold\"]\n",
        "    for review in review_grid:\n",
        "        for demote in demote_grid:\n",
        "            if demote <= review:\n",
        "                continue\n",
        "            y_pred = []\n",
        "            for row in df.itertuples(index=False):\n",
        "                if bool(row.is_spam_pred) and row.confidence_pred >= demote:\n",
        "                    y_pred.append(1)\n",
        "                elif bool(row.is_spam_pred) and row.confidence_pred >= review:\n",
        "                    y_pred.append(1)\n",
        "                else:\n",
        "                    y_pred.append(0)\n",
        "            score = precision_recall_fscore_support(df[\"label\"], y_pred, average=\"macro\", zero_division=0)[2]\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_review = review\n",
        "                best_demote = demote\n",
        "    return best_review, best_demote, best_score\n",
        "\n",
        "\n",
        "review_candidates = np.linspace(0.1, 0.9, num=9)\n",
        "demote_candidates = np.linspace(0.2, 1.0, num=9)\n",
        "best_review, best_demote, best_score = sweep_thresholds(val_predictions, review_candidates, demote_candidates)\n",
        "print(f\"Best thresholds: review={best_review:.2f}, demote={best_demote:.2f}, macro_f1={best_score:.3f}\")\n",
        "\n",
        "metrics = evaluate_predictions(val_predictions, best_review, best_demote)\n",
        "print(json.dumps(metrics, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Curated gallery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title TP/TN/FP/FN examples with images\n",
        "import base64\n",
        "import itertools\n",
        "\n",
        "CATEGORY_MAP = {\n",
        "    \"True Positive\": lambda r: r.label == 1 and r.is_spam_pred,\n",
        "    \"True Negative\": lambda r: r.label == 0 and not r.is_spam_pred,\n",
        "    \"False Positive\": lambda r: r.label == 0 and r.is_spam_pred,\n",
        "    \"False Negative\": lambda r: r.label == 1 and not r.is_spam_pred,\n",
        "}\n",
        "\n",
        "GALLERY_MAX = 4\n",
        "\n",
        "\n",
        "def image_to_base64(path: Path | None) -> str:\n",
        "    if path is None or not path.exists():\n",
        "        return \"\"\n",
        "    return base64.b64encode(path.read_bytes()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def display_gallery(df: pd.DataFrame) -> None:\n",
        "    rows = []\n",
        "    for category, predicate in CATEGORY_MAP.items():\n",
        "        subset = [r for r in df.itertuples(index=False) if predicate(r)]\n",
        "        for row in itertools.islice(subset, GALLERY_MAX):\n",
        "            original = train_df.loc[train_df.product_id == row.product_id].iloc[0]\n",
        "            img_path = utils.download_image(original.get(\"image_url\"), CONFIG[\"cache_dir\"])\n",
        "            encoded = image_to_base64(img_path)\n",
        "            img_tag = f\"<img src='data:image/jpeg;base64,{encoded}' width='200'>\" if encoded else \"(no image)\"\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"Category\": category,\n",
        "                    \"Product ID\": row.product_id,\n",
        "                    \"True Label\": row.label,\n",
        "                    \"Predicted\": row.is_spam_pred,\n",
        "                    \"Confidence\": f\"{row.confidence_pred:.2f}\",\n",
        "                    \"Reason\": row.reason_pred,\n",
        "                    \"Image\": img_tag,\n",
        "                }\n",
        "            )\n",
        "    if not rows:\n",
        "        print(\"No rows to display\")\n",
        "        return\n",
        "    display(HTML(pd.DataFrame(rows).to_html(escape=False)))\n",
        "\n",
        "\n",
        "display_gallery(val_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Package artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Bundle outputs for download\n",
        "import shutil\n",
        "\n",
        "zip_path = Path(CONFIG[\"artifacts_dir\"]) / \"keyword_spam_artifacts.zip\"\n",
        "if zip_path.exists():\n",
        "    zip_path.unlink()\n",
        "shutil.make_archive(str(zip_path.with_suffix(\"\")), \"zip\", CONFIG[\"artifacts_dir\"])\n",
        "print(f\"Artifacts packaged at {zip_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
